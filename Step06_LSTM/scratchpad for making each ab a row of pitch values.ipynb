{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "infile = open('../data/processed/scherzer_with_batters.pickle','rb')\n",
    "pb = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open('../data/processed/X_train_trans.pickle','rb')\n",
    "X_train = pickle.load(infile)\n",
    "infile.close()\n",
    "\n",
    "infile = open('../data/processed/y_train_trans.pickle','rb')\n",
    "y_train = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb.batter_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36000"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "200 * 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# \"Sunny day went to ....\" picnic? park?\n",
    "\n",
    "# NLP problem is everywhere\n",
    "# Ng work with shakespeare\n",
    "\n",
    "# Our book is -> each sequence is at bat (pitcher vs. batter)\n",
    "\n",
    "# 'Schzer threw a ff to batter ast the first pitch.  The next pitch xyz, the pitch after was '\n",
    "\n",
    "# Murat suggests -> find LSTM tutorial with text guessing -> \n",
    "# will do same thing but with pitches -> \n",
    "# their data with pitch data (skip tokenization etc.)\n",
    "\n",
    "# Look at how they pass the data\n",
    "\n",
    "# Keras will pad null pitches for an at bat\n",
    "\n",
    "# Each row is an at bat -> need to figure out what the numbers will be\n",
    "\n",
    "# Each row is an at bat, padded by max length, sequence is pitch type.\n",
    "\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pitch_type', 'game_date_x', 'sv_id', 'batter_id', 'pitch_number',\n",
       "       'release_speed', 'zone', 'stand', 'home_team', 'on_3b', 'on_2b',\n",
       "       'on_1b', 'outs_when_up', 'inning', 'release_spin_rate', 'opp_score',\n",
       "       'nats_score', 'if_fielding_alignment', 'of_fielding_alignment',\n",
       "       'nats_home1_away0', 'balls_strikes', 'all_runners', 'pitch_season',\n",
       "       'pitch_game', 'pitch_bat_gm', 'game_date_y', 'shift_date',\n",
       "       'player_name', 'total_pitches', 'hits', 'abs', 'whiffs', 'swings',\n",
       "       'takes', 'k', 'walk', 'single', 'double', 'triple', 'hr', 'line_drive',\n",
       "       'ground_ball', 'fly_ball', 'popup', 'rbi', 'sac', 'ba', 'slg', 'iso',\n",
       "       'babip'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "pitch_vals = pb.pitch_type.unique().tolist()\n",
    "\n",
    "pitch_vals = dict(zip(pitch_vals, range(1, len(pitch_vals) + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pb_df = pb[['pitch_type', 'pitch_number', 'pitch_bat_gm']][:21]\n",
    "# pb_df;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pitch_type</th>\n",
       "      <th>game_date_x</th>\n",
       "      <th>sv_id</th>\n",
       "      <th>batter_id</th>\n",
       "      <th>pitch_number</th>\n",
       "      <th>release_speed</th>\n",
       "      <th>zone</th>\n",
       "      <th>stand</th>\n",
       "      <th>home_team</th>\n",
       "      <th>on_3b</th>\n",
       "      <th>...</th>\n",
       "      <th>fly_ball</th>\n",
       "      <th>popup</th>\n",
       "      <th>rbi</th>\n",
       "      <th>sac</th>\n",
       "      <th>ba</th>\n",
       "      <th>slg</th>\n",
       "      <th>iso</th>\n",
       "      <th>babip</th>\n",
       "      <th>new_ab</th>\n",
       "      <th>pitch_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FF</td>\n",
       "      <td>2019-03-28</td>\n",
       "      <td>190328_170717</td>\n",
       "      <td>607043</td>\n",
       "      <td>0</td>\n",
       "      <td>93.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>L</td>\n",
       "      <td>WSH</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FF</td>\n",
       "      <td>2019-03-28</td>\n",
       "      <td>190328_170732</td>\n",
       "      <td>607043</td>\n",
       "      <td>1</td>\n",
       "      <td>94.2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>L</td>\n",
       "      <td>WSH</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FF</td>\n",
       "      <td>2019-03-28</td>\n",
       "      <td>190328_170752</td>\n",
       "      <td>607043</td>\n",
       "      <td>2</td>\n",
       "      <td>96.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>L</td>\n",
       "      <td>WSH</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SL</td>\n",
       "      <td>2019-03-28</td>\n",
       "      <td>190328_170825</td>\n",
       "      <td>624413</td>\n",
       "      <td>0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>R</td>\n",
       "      <td>WSH</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FF</td>\n",
       "      <td>2019-03-28</td>\n",
       "      <td>190328_170842</td>\n",
       "      <td>624413</td>\n",
       "      <td>1</td>\n",
       "      <td>95.5</td>\n",
       "      <td>12.0</td>\n",
       "      <td>R</td>\n",
       "      <td>WSH</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>CU</td>\n",
       "      <td>2019-09-24</td>\n",
       "      <td>190925_005705</td>\n",
       "      <td>664068</td>\n",
       "      <td>1</td>\n",
       "      <td>79.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>R</td>\n",
       "      <td>WSH</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0.259009</td>\n",
       "      <td>0.481982</td>\n",
       "      <td>0.222973</td>\n",
       "      <td>0.334495</td>\n",
       "      <td>no</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2766</th>\n",
       "      <td>SL</td>\n",
       "      <td>2019-09-24</td>\n",
       "      <td>190925_005726</td>\n",
       "      <td>664068</td>\n",
       "      <td>2</td>\n",
       "      <td>84.8</td>\n",
       "      <td>14.0</td>\n",
       "      <td>R</td>\n",
       "      <td>WSH</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0.259009</td>\n",
       "      <td>0.481982</td>\n",
       "      <td>0.222973</td>\n",
       "      <td>0.334495</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>FF</td>\n",
       "      <td>2019-09-24</td>\n",
       "      <td>190925_005750</td>\n",
       "      <td>664068</td>\n",
       "      <td>3</td>\n",
       "      <td>97.4</td>\n",
       "      <td>8.0</td>\n",
       "      <td>R</td>\n",
       "      <td>WSH</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0.259009</td>\n",
       "      <td>0.481982</td>\n",
       "      <td>0.222973</td>\n",
       "      <td>0.334495</td>\n",
       "      <td>no</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2768</th>\n",
       "      <td>CH</td>\n",
       "      <td>2019-09-24</td>\n",
       "      <td>190925_005823</td>\n",
       "      <td>664068</td>\n",
       "      <td>4</td>\n",
       "      <td>84.4</td>\n",
       "      <td>9.0</td>\n",
       "      <td>R</td>\n",
       "      <td>WSH</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>76</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>0.259009</td>\n",
       "      <td>0.481982</td>\n",
       "      <td>0.222973</td>\n",
       "      <td>0.334495</td>\n",
       "      <td>no</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2769</th>\n",
       "      <td>FF</td>\n",
       "      <td>2019-09-24</td>\n",
       "      <td>190925_005920</td>\n",
       "      <td>656514</td>\n",
       "      <td>0</td>\n",
       "      <td>92.7</td>\n",
       "      <td>6.0</td>\n",
       "      <td>L</td>\n",
       "      <td>WSH</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.268041</td>\n",
       "      <td>0.412371</td>\n",
       "      <td>0.144330</td>\n",
       "      <td>0.345588</td>\n",
       "      <td>yes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2770 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     pitch_type game_date_x          sv_id  batter_id  pitch_number  \\\n",
       "0            FF  2019-03-28  190328_170717     607043             0   \n",
       "1            FF  2019-03-28  190328_170732     607043             1   \n",
       "2            FF  2019-03-28  190328_170752     607043             2   \n",
       "3            SL  2019-03-28  190328_170825     624413             0   \n",
       "4            FF  2019-03-28  190328_170842     624413             1   \n",
       "...         ...         ...            ...        ...           ...   \n",
       "2765         CU  2019-09-24  190925_005705     664068             1   \n",
       "2766         SL  2019-09-24  190925_005726     664068             2   \n",
       "2767         FF  2019-09-24  190925_005750     664068             3   \n",
       "2768         CH  2019-09-24  190925_005823     664068             4   \n",
       "2769         FF  2019-09-24  190925_005920     656514             0   \n",
       "\n",
       "      release_speed  zone stand home_team  on_3b  ...  fly_ball  popup  rbi  \\\n",
       "0              93.7   6.0     L       WSH      0  ...         0      0    0   \n",
       "1              94.2   5.0     L       WSH      0  ...         0      0    0   \n",
       "2              96.3   5.0     L       WSH      0  ...         0      0    0   \n",
       "3              85.6   6.0     R       WSH      0  ...         0      0    0   \n",
       "4              95.5  12.0     R       WSH      0  ...         0      0    0   \n",
       "...             ...   ...   ...       ...    ...  ...       ...    ...  ...   \n",
       "2765           79.1   4.0     R       WSH      0  ...        76     27   36   \n",
       "2766           84.8  14.0     R       WSH      0  ...        76     27   36   \n",
       "2767           97.4   8.0     R       WSH      0  ...        76     27   36   \n",
       "2768           84.4   9.0     R       WSH      0  ...        76     27   36   \n",
       "2769           92.7   6.0     L       WSH      0  ...        22      7   19   \n",
       "\n",
       "      sac        ba       slg       iso     babip new_ab  pitch_val  \n",
       "0       0  0.000000  0.000000  0.000000  0.000000    yes          1  \n",
       "1       0  0.000000  0.000000  0.000000  0.000000     no          1  \n",
       "2       0  0.000000  0.000000  0.000000  0.000000     no          1  \n",
       "3       0  0.000000  0.000000  0.000000  0.000000    yes          2  \n",
       "4       0  0.000000  0.000000  0.000000  0.000000     no          1  \n",
       "...   ...       ...       ...       ...       ...    ...        ...  \n",
       "2765    3  0.259009  0.481982  0.222973  0.334495     no          5  \n",
       "2766    3  0.259009  0.481982  0.222973  0.334495     no          2  \n",
       "2767    3  0.259009  0.481982  0.222973  0.334495     no          1  \n",
       "2768    3  0.259009  0.481982  0.222973  0.334495     no          4  \n",
       "2769    0  0.268041  0.412371  0.144330  0.345588    yes          1  \n",
       "\n",
       "[2770 rows x 52 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pb.loc[(pb.pitch_number == 0), 'new_ab'] = 'yes'\n",
    "pb.new_ab = pb.new_ab.fillna(value='no')\n",
    "\n",
    "pb['pitch_val'] = pb.pitch_type.apply(lambda p: pitch_vals[p])\n",
    "# pb_df = pb_df[['pitch_type', 'pitch_val', 'pitch_number', 'pitch_bat_gm', 'new_ab']]\n",
    "\n",
    "pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1],\n",
       " [2, 1, 1, 3, 1, 4, 3],\n",
       " [1, 4, 4],\n",
       " [5, 1, 1, 4],\n",
       " [1, 2, 1, 4, 5],\n",
       " [2, 1, 2, 1],\n",
       " [1, 1, 1, 4],\n",
       " [4, 1, 1],\n",
       " [1, 5, 1, 1, 4, 3],\n",
       " [2, 1, 1, 1, 4],\n",
       " [1, 1, 3],\n",
       " [4, 1, 5, 1, 1, 4, 1],\n",
       " [1, 2],\n",
       " [1, 1],\n",
       " [2, 4, 2, 2, 4, 2],\n",
       " [1, 4, 1],\n",
       " [4, 4, 1],\n",
       " [1, 2, 1],\n",
       " [1, 5, 3],\n",
       " [4, 4, 4, 1, 5, 3, 1],\n",
       " [2, 4],\n",
       " [1, 1, 4, 4, 1, 5],\n",
       " [1, 4],\n",
       " [1, 2, 2, 2, 1],\n",
       " [5, 1, 3, 4, 1, 1, 4],\n",
       " [1, 5, 4, 4],\n",
       " [1, 1],\n",
       " [2, 1],\n",
       " [1, 3, 1, 3, 4, 4],\n",
       " [1, 2, 1, 1, 1, 1, 2],\n",
       " [2, 2, 2],\n",
       " [1, 3, 1, 4, 1],\n",
       " [1, 1, 5, 1, 1],\n",
       " [2, 1, 1, 5, 1, 4],\n",
       " [1, 2, 2],\n",
       " [2, 1, 5],\n",
       " [5, 2, 2],\n",
       " [3, 1, 1, 1, 1, 4, 3],\n",
       " [1, 4, 2, 1, 4, 1],\n",
       " [1, 1, 4, 1],\n",
       " [1, 5, 1, 5, 5],\n",
       " [1, 4],\n",
       " [1, 2, 1, 1, 2, 2, 1],\n",
       " [1, 2, 1, 4],\n",
       " [2, 1, 4],\n",
       " [4, 3, 1, 1, 5],\n",
       " [4, 2, 4],\n",
       " [5, 2, 2, 4, 1],\n",
       " [1, 1, 1, 3],\n",
       " [1, 1, 1, 4, 4, 1],\n",
       " [1, 1, 4, 3, 1, 1],\n",
       " [1, 1, 1, 4, 4],\n",
       " [1, 1, 2, 1, 2, 2],\n",
       " [1, 2, 2, 2],\n",
       " [1, 1, 2, 1],\n",
       " [1, 1],\n",
       " [1, 5, 4, 3, 4, 1],\n",
       " [2, 1, 1, 1],\n",
       " [2, 1, 1],\n",
       " [5, 5, 3, 4, 1, 1, 4],\n",
       " [1, 1, 3],\n",
       " [2, 2, 1, 1],\n",
       " [1, 2, 4, 4, 2],\n",
       " [1, 2],\n",
       " [1, 1, 1, 2, 1],\n",
       " [1, 4],\n",
       " [1, 3, 1],\n",
       " [1, 2, 2, 1],\n",
       " [1, 1, 2],\n",
       " [1, 1],\n",
       " [2, 1, 1, 1, 1, 2, 1],\n",
       " [1, 1, 1],\n",
       " [1, 3],\n",
       " [2, 1, 2],\n",
       " [1, 5],\n",
       " [1, 1, 4, 3, 1],\n",
       " [3, 1, 4, 1],\n",
       " [1, 1, 4, 2],\n",
       " [1, 1, 2],\n",
       " [4, 1, 4, 4, 1],\n",
       " [1, 4, 1],\n",
       " [2, 1, 4, 3, 1],\n",
       " [5, 5, 1, 1, 3],\n",
       " [4, 1, 1, 4, 3, 1, 1],\n",
       " [1, 2, 1, 2],\n",
       " [1, 2],\n",
       " [5, 1, 4],\n",
       " [2, 1, 1, 2, 2],\n",
       " [5, 1, 4, 4],\n",
       " [5, 4, 4, 2, 1],\n",
       " [4, 1],\n",
       " [2, 1, 5],\n",
       " [1, 5, 1],\n",
       " [1, 1, 4, 2, 2, 1],\n",
       " [5, 1, 4, 1, 5],\n",
       " [4, 1],\n",
       " [2, 4, 2, 4],\n",
       " [1, 2],\n",
       " [5, 1, 3, 1, 4],\n",
       " [1, 1, 2, 3, 4, 2],\n",
       " [1, 5],\n",
       " [1, 3, 1, 2],\n",
       " [1, 1, 2],\n",
       " [1, 1, 2, 2, 1],\n",
       " [1, 2, 1, 2, 1],\n",
       " [1, 5, 1, 1, 5, 1],\n",
       " [1, 1, 4, 1, 2],\n",
       " [1, 3, 4, 1, 3],\n",
       " [1, 1, 4, 1, 2, 2],\n",
       " [2, 2, 1, 1, 1, 1, 2],\n",
       " [2, 2, 3],\n",
       " [2, 1],\n",
       " [1, 2, 4, 1],\n",
       " [5, 1, 5, 1, 1],\n",
       " [1, 2, 4, 1],\n",
       " [2, 1, 1, 1, 2],\n",
       " [5, 4],\n",
       " [1, 1, 1, 1, 2, 4, 2, 1],\n",
       " [2, 1, 2, 4, 1],\n",
       " [5, 4],\n",
       " [2, 2, 1],\n",
       " [4, 1, 3, 4, 1, 5],\n",
       " [1, 2, 1, 2, 2],\n",
       " [1, 1, 2],\n",
       " [2, 1, 1, 1, 2, 2, 1],\n",
       " [1, 5, 4, 3],\n",
       " [1, 1, 2, 1, 4, 1],\n",
       " [2, 1, 2],\n",
       " [1, 1, 1, 4, 2],\n",
       " [1, 2, 1, 1],\n",
       " [1, 2, 1, 4],\n",
       " [5, 2, 1, 1],\n",
       " [1, 4, 1, 1, 2],\n",
       " [1, 2],\n",
       " [5, 3, 1],\n",
       " [2, 1, 4, 2, 1, 4],\n",
       " [5, 2, 1],\n",
       " [1, 4, 2],\n",
       " [2, 1, 2, 2],\n",
       " [1, 4, 4, 2],\n",
       " [2, 2, 1, 4],\n",
       " [1, 2, 1, 2, 2],\n",
       " [2, 1, 4],\n",
       " [3, 1, 1, 4, 3, 1],\n",
       " [1, 4, 2, 1, 2, 1, 1, 1],\n",
       " [1, 1, 1, 1, 4, 1],\n",
       " [2, 1],\n",
       " [1, 1, 1],\n",
       " [1, 2, 2, 1, 1],\n",
       " [1, 4],\n",
       " [1, 4, 1],\n",
       " [1, 4, 2, 2, 1],\n",
       " [1, 1],\n",
       " [3, 4, 5, 1, 1, 3, 2],\n",
       " [1, 2, 2, 1, 1],\n",
       " [1, 4, 2],\n",
       " [2, 1, 2, 1],\n",
       " [1, 5],\n",
       " [5, 1, 1, 1],\n",
       " [2, 1, 1, 1, 4, 2],\n",
       " [1, 2, 2],\n",
       " [1, 5, 1, 4, 4, 1],\n",
       " [5, 2, 2, 2, 4],\n",
       " [2, 1],\n",
       " [1, 1],\n",
       " [2, 1, 4, 1],\n",
       " [4, 1, 2],\n",
       " [1, 5, 1, 1, 4, 3],\n",
       " [2, 2, 2, 4, 4, 1, 1],\n",
       " [2, 2, 4, 2],\n",
       " [1, 1],\n",
       " [1, 2, 1, 2, 2, 1, 1],\n",
       " [1, 3, 1, 1],\n",
       " [3, 1, 3, 1, 1],\n",
       " [1, 5, 4],\n",
       " [3, 1, 1, 4, 1],\n",
       " [1, 5, 1, 5],\n",
       " [1, 5, 1, 4, 2],\n",
       " [3, 1, 1, 5],\n",
       " [1, 2, 1, 2],\n",
       " [2, 2, 1, 1, 2, 1],\n",
       " [5, 4, 1],\n",
       " [5, 4, 3, 1, 4, 1, 1],\n",
       " [1, 1],\n",
       " [3, 4, 3, 1],\n",
       " [1, 4, 1, 1],\n",
       " [1, 2, 4, 4],\n",
       " [5, 4, 3],\n",
       " [1, 1, 1, 4],\n",
       " [1, 1],\n",
       " [1, 5, 1, 1, 5, 3, 3, 3, 1],\n",
       " [1, 4, 4],\n",
       " [5, 3, 4, 4],\n",
       " [1, 2, 4],\n",
       " [5, 1, 3, 4, 3],\n",
       " [1, 1, 1, 1],\n",
       " [1, 1, 1, 1],\n",
       " [1, 2, 2, 2, 1, 1, 1],\n",
       " [1, 1, 3, 1],\n",
       " [5, 1, 1, 1, 3, 1],\n",
       " [1, 4],\n",
       " [5, 1, 1, 4, 1, 1],\n",
       " [2, 1, 1],\n",
       " [1, 1, 2, 2, 1],\n",
       " [1, 2, 2, 1],\n",
       " [1, 3],\n",
       " [1, 5, 4, 4, 1, 4],\n",
       " [2, 2, 2, 1],\n",
       " [5, 1, 1, 3],\n",
       " [4, 1],\n",
       " [4, 5, 1, 4, 4, 1, 3],\n",
       " [1, 1, 2, 2, 2],\n",
       " [2, 1, 1, 1],\n",
       " [5, 4, 1, 3, 4],\n",
       " [1, 5, 1, 4, 1],\n",
       " [2, 1, 1, 2, 2],\n",
       " [5, 1, 1, 3, 1, 3, 4, 1],\n",
       " [1, 5, 1, 3],\n",
       " [1, 1, 1, 3, 4],\n",
       " [2, 1, 4, 2, 2, 1],\n",
       " [2, 1, 1],\n",
       " [2, 2, 1, 5],\n",
       " [1, 4, 1, 1, 1, 1],\n",
       " [5, 1, 3, 1, 1],\n",
       " [3, 3, 1, 3, 4],\n",
       " [1, 1, 2, 2],\n",
       " [1, 1, 4, 2],\n",
       " [1, 5],\n",
       " [1, 5, 5, 1, 4, 3, 1],\n",
       " [2, 2, 4],\n",
       " [1, 5, 3, 1],\n",
       " [3, 1, 1, 3, 4, 3],\n",
       " [2, 2, 1, 2],\n",
       " [1, 1, 3, 1],\n",
       " [2, 1, 1, 1, 1, 1],\n",
       " [1, 2, 2],\n",
       " [5, 1, 3],\n",
       " [5, 2, 1, 4, 3, 1],\n",
       " [1, 2, 2, 2, 1, 1, 1],\n",
       " [3, 3, 4],\n",
       " [1, 5, 3, 1, 1, 5],\n",
       " [2, 1, 1, 1, 1, 2],\n",
       " [1, 4],\n",
       " [1, 2, 2, 2],\n",
       " [1, 2, 1, 1, 2],\n",
       " [3, 5, 3, 4],\n",
       " [1, 3, 1, 1, 1, 5, 3, 1, 2],\n",
       " [1, 1, 1, 1, 1],\n",
       " [2, 1, 1, 2, 1, 4, 2],\n",
       " [1, 3, 1, 4, 1],\n",
       " [1, 1, 2, 2, 1],\n",
       " [1, 1, 3],\n",
       " [1, 2],\n",
       " [1, 1],\n",
       " [2, 1, 2],\n",
       " [1, 4],\n",
       " [2, 1, 2],\n",
       " [2, 1, 1, 4, 4, 1, 1],\n",
       " [3, 3, 4, 1, 4],\n",
       " [1, 1, 2, 1, 1, 5],\n",
       " [2, 1],\n",
       " [1, 2, 3, 1, 2, 2, 1, 2, 1],\n",
       " [2, 2, 2, 1],\n",
       " [2, 2, 2, 4],\n",
       " [2, 2, 1, 4],\n",
       " [1, 1, 4, 4, 1, 1, 2, 2],\n",
       " [2, 1, 1, 1, 2, 1, 1],\n",
       " [2, 1, 2, 5, 1],\n",
       " [2, 2, 1, 4, 2],\n",
       " [1, 2],\n",
       " [2, 2, 1, 1],\n",
       " [1, 1],\n",
       " [1, 1, 2, 2],\n",
       " [1, 1, 1, 2],\n",
       " [5, 3, 1, 1],\n",
       " [1, 1, 1],\n",
       " [5, 1, 2, 1, 2, 5],\n",
       " [1, 2],\n",
       " [1, 1, 2],\n",
       " [2, 1, 2],\n",
       " [5, 3, 5, 4, 3, 3, 5, 1, 3, 1, 1, 4],\n",
       " [5, 1, 1, 4, 1, 4, 2],\n",
       " [2, 1, 1, 1],\n",
       " [3, 1, 4, 2],\n",
       " [1, 1, 2, 2],\n",
       " [2, 1, 5, 3, 4],\n",
       " [1, 2, 1, 2],\n",
       " [2, 5],\n",
       " [1, 4, 4, 1, 5, 1, 1, 1, 2],\n",
       " [1, 1],\n",
       " [1, 3, 3],\n",
       " [1, 1, 2, 2, 4, 1, 1, 2, 1],\n",
       " [2, 2, 1, 2, 1],\n",
       " [1, 2],\n",
       " [1, 2, 2, 1, 1],\n",
       " [5, 1],\n",
       " [5, 1],\n",
       " [1, 3],\n",
       " [1, 2, 1],\n",
       " [5, 1, 1, 4, 4],\n",
       " [2, 1, 1, 1, 2, 1, 1, 1],\n",
       " [1, 5, 3, 3],\n",
       " [2, 1, 3, 2],\n",
       " [2, 1, 1],\n",
       " [1, 1, 3, 1, 3, 5],\n",
       " [1, 3],\n",
       " [1, 3, 3],\n",
       " [2, 1],\n",
       " [5, 1, 1, 4, 3, 1, 4],\n",
       " [5, 1, 1],\n",
       " [4, 1, 4, 1, 5, 1, 1, 4],\n",
       " [1, 2, 1, 4, 1, 4],\n",
       " [5, 1],\n",
       " [3, 1, 5, 5, 4, 1, 1],\n",
       " [5, 4, 1, 1],\n",
       " [4, 5, 1, 4, 3, 1],\n",
       " [2, 1, 1, 1],\n",
       " [1, 3, 1, 4, 4, 1, 1],\n",
       " [1, 1, 1],\n",
       " [1, 2, 2, 1, 4, 1, 4, 1, 2],\n",
       " [5, 1, 1],\n",
       " [1, 1, 3, 1, 1],\n",
       " [2, 1, 1, 2, 1, 2, 4, 2],\n",
       " [2, 1, 1, 1, 2, 2],\n",
       " [1, 5, 3],\n",
       " [2, 1, 1, 1, 2],\n",
       " [1, 4, 3],\n",
       " [1, 1],\n",
       " [2, 1, 2, 2],\n",
       " [5, 1],\n",
       " [5, 2, 4, 1],\n",
       " [1, 2, 5, 4],\n",
       " [3, 1],\n",
       " [1, 2, 1],\n",
       " [1, 2, 4, 2, 1, 2, 1],\n",
       " [5, 1, 1],\n",
       " [1, 2, 1],\n",
       " [1, 3],\n",
       " [1, 1, 4, 2, 2, 1, 5],\n",
       " [2, 1, 1],\n",
       " [5, 5, 3, 4],\n",
       " [1, 1, 1, 1],\n",
       " [5, 4],\n",
       " [2, 1, 4, 2],\n",
       " [1, 5],\n",
       " [2, 1, 5, 2, 1, 1],\n",
       " [1, 3, 5],\n",
       " [1, 1, 1, 1],\n",
       " [1, 1, 4, 1, 4],\n",
       " [1, 3],\n",
       " [1, 2, 2, 2],\n",
       " [1, 1, 3, 4],\n",
       " [1, 1, 2],\n",
       " [2, 1, 2, 1],\n",
       " [2, 1],\n",
       " [1, 5, 1],\n",
       " [5, 5, 1, 1, 3, 4, 1],\n",
       " [1, 4, 5],\n",
       " [1, 1],\n",
       " [1, 1, 1, 1, 1],\n",
       " [1, 2, 1, 1],\n",
       " [2, 2, 1, 4, 4, 2],\n",
       " [5, 5, 4, 1],\n",
       " [4, 1, 3, 5, 1, 4, 1, 1, 1],\n",
       " [4, 1, 1, 4],\n",
       " [4, 1],\n",
       " [2, 2, 2, 1, 1, 2, 4],\n",
       " [4, 1, 4],\n",
       " [2, 2, 1],\n",
       " [2, 2, 2, 1, 4, 2],\n",
       " [2, 1],\n",
       " [5, 1, 4, 1, 1, 1],\n",
       " [2, 1, 1, 2],\n",
       " [1, 1, 3, 1],\n",
       " [1, 2],\n",
       " [2, 2, 1, 1, 1, 1, 2, 3],\n",
       " [1, 5, 1, 4, 4, 1, 1],\n",
       " [5, 1, 1, 3],\n",
       " [1, 5, 1, 5, 1, 1],\n",
       " [1, 2, 1],\n",
       " [1, 1, 5],\n",
       " [4, 1, 1, 1, 1, 3],\n",
       " [5, 2, 1, 2, 1],\n",
       " [5, 1, 2],\n",
       " [1, 1, 4, 3, 4],\n",
       " [1, 4, 1, 3],\n",
       " [1, 1, 1],\n",
       " [5, 1, 4],\n",
       " [2, 1, 2, 1, 2, 5],\n",
       " [4, 1, 4, 3, 3, 4],\n",
       " [1, 1, 4, 4, 2, 2],\n",
       " [1, 2, 1],\n",
       " [4, 1, 4, 1, 3, 1],\n",
       " [1, 1, 5, 3, 3],\n",
       " [1, 1, 1, 1, 4, 1],\n",
       " [1, 5, 1, 1, 1],\n",
       " [2, 1, 1, 2],\n",
       " [1, 1, 1, 2, 1, 1, 1, 2, 1],\n",
       " [2, 1, 2],\n",
       " [1, 1],\n",
       " [5, 1, 1, 4, 3],\n",
       " [5, 3, 1, 3, 5, 4],\n",
       " [1, 1, 3, 4, 1],\n",
       " [1, 2, 2],\n",
       " [1, 2, 1, 4],\n",
       " [2, 1, 1, 2],\n",
       " [2, 1],\n",
       " [2, 4],\n",
       " [1, 4],\n",
       " [5, 4],\n",
       " [5, 1, 3],\n",
       " [1, 3, 4],\n",
       " [1, 2, 2, 1, 1, 1, 1, 2],\n",
       " [1, 2],\n",
       " [1, 1, 4],\n",
       " [1, 5, 3],\n",
       " [1, 5, 1, 4, 4],\n",
       " [1, 4, 1, 3],\n",
       " [1, 1, 4, 2, 2],\n",
       " [2, 4, 2, 2],\n",
       " [2, 2, 2, 1, 2],\n",
       " [1, 1, 2, 1],\n",
       " [2, 1, 2],\n",
       " [1, 1, 1, 4, 3],\n",
       " [1, 5, 5, 4, 3],\n",
       " [2, 1],\n",
       " [1, 4, 4, 1, 3, 4],\n",
       " [1, 1, 4, 3, 1, 4, 1, 5, 3, 1],\n",
       " [1, 1, 2, 4],\n",
       " [1, 2, 2, 1],\n",
       " [2, 2],\n",
       " [2, 2, 4, 2, 2, 1, 1, 2],\n",
       " [5, 1, 1, 1],\n",
       " [5, 4, 1],\n",
       " [1, 2, 4],\n",
       " [5, 1, 3, 3, 4],\n",
       " [1, 1, 3, 4],\n",
       " [2, 1],\n",
       " [2, 1, 1, 1, 2],\n",
       " [1, 2, 4, 4, 2, 2],\n",
       " [1, 5, 5, 1],\n",
       " [1, 4],\n",
       " [2, 1],\n",
       " [1, 1, 4],\n",
       " [5, 4, 3, 3],\n",
       " [2, 2, 1, 2, 2, 1],\n",
       " [2, 1, 1, 2],\n",
       " [1, 2, 1, 2, 4],\n",
       " [1, 2, 1],\n",
       " [1, 1, 4, 2, 1, 4],\n",
       " [1, 4, 1, 1, 1, 3, 4, 1],\n",
       " [1, 1, 1, 2, 1],\n",
       " [2, 2, 2],\n",
       " [1, 5, 3, 4],\n",
       " [1, 3, 1, 3],\n",
       " [2, 1],\n",
       " [1, 2],\n",
       " [1, 2, 1, 4, 2, 1],\n",
       " [3, 4, 4, 3],\n",
       " [1, 5, 1, 3, 3, 1],\n",
       " [2, 1],\n",
       " [1, 2, 1, 2],\n",
       " [5, 4, 1, 5, 1],\n",
       " [5, 1],\n",
       " [1, 2],\n",
       " [1, 2, 2, 2],\n",
       " [5, 2, 2, 2, 1, 1],\n",
       " [1, 1, 5, 1],\n",
       " [1, 1, 3, 5, 4],\n",
       " [1, 1, 1, 2],\n",
       " [1, 4, 1, 3],\n",
       " [5, 4, 2, 2],\n",
       " [2, 2, 1, 1],\n",
       " [1, 1, 3],\n",
       " [1, 2, 1, 1, 2, 1],\n",
       " [5, 1, 1, 4, 3, 3, 1, 1],\n",
       " [1, 5, 1],\n",
       " [1, 1, 1, 1, 1, 1, 4],\n",
       " [1, 2, 1, 1, 2, 1],\n",
       " [1, 1, 1, 1, 1, 1],\n",
       " [1, 4, 4],\n",
       " [5, 2, 1],\n",
       " [1, 5, 1, 4, 4],\n",
       " [1, 5, 1, 1, 3, 3],\n",
       " [1, 4, 4],\n",
       " [1, 1, 4],\n",
       " [2, 2, 1, 1],\n",
       " [5, 1, 1, 4],\n",
       " [1, 2, 2],\n",
       " [1, 1, 5, 1, 1],\n",
       " [1, 1, 2, 2, 2, 1, 2, 5],\n",
       " [1, 1, 4, 3, 1, 3],\n",
       " [1, 4, 4, 3, 1, 1],\n",
       " [2, 1, 1, 2, 2, 1, 1, 1],\n",
       " [1, 1, 4, 3],\n",
       " [1, 4],\n",
       " [2, 1, 2, 2],\n",
       " [2, 1, 1, 2, 2],\n",
       " [1, 5, 1],\n",
       " [5, 1, 4],\n",
       " [1, 2, 2, 4, 2],\n",
       " [4, 4, 1, 1, 1, 3, 1],\n",
       " [1, 1, 1, 1, 5, 1, 1, 1],\n",
       " [1, 5, 2],\n",
       " [2, 2, 2, 1, 1, 2, 4],\n",
       " [1, 1, 3, 4],\n",
       " [1, 1, 4, 2],\n",
       " [1, 5, 1, 4, 4, 1],\n",
       " [1, 5, 5],\n",
       " [5, 1, 4, 1],\n",
       " [1, 1, 5, 1, 4, 4, 1],\n",
       " [1, 1],\n",
       " [1, 2, 1, 1, 2, 1],\n",
       " [4, 1, 1, 1, 3],\n",
       " [1, 4, 1, 1, 4],\n",
       " [1, 1, 2, 1],\n",
       " [5, 1, 4, 3],\n",
       " [5, 1, 3, 3, 5, 5],\n",
       " [1, 4, 4, 1, 1, 3, 3],\n",
       " [1, 4, 1, 3, 1],\n",
       " [5, 1, 1, 1, 1, 3, 1],\n",
       " [5, 1, 3, 4, 5],\n",
       " [1, 4, 3, 4, 1],\n",
       " [1, 1, 4, 1, 4],\n",
       " [1, 2, 2, 1, 1],\n",
       " [5, 1, 5, 1, 1, 3],\n",
       " [1, 1, 2, 1],\n",
       " [1, 1, 4, 4, 1, 5, 1],\n",
       " [1, 1, 4, 4, 1],\n",
       " [2, 2, 2, 2, 1, 4],\n",
       " [1, 1, 4],\n",
       " [1, 5, 4],\n",
       " [4, 3, 2, 1, 1],\n",
       " [1, 1, 1, 1, 1],\n",
       " [1, 2, 1],\n",
       " [5, 4],\n",
       " [5, 3, 3, 5, 1, 3, 1, 1],\n",
       " [3, 4, 4],\n",
       " [5, 2],\n",
       " [1, 4, 1, 3],\n",
       " [2, 2],\n",
       " [5, 5, 3, 1, 3, 3],\n",
       " [1, 1, 4, 4, 1, 5],\n",
       " [2, 1],\n",
       " [1, 5, 1, 1, 3, 1],\n",
       " [1, 1, 4, 4, 1, 1],\n",
       " [1, 1, 1, 2, 3, 2, 1, 1],\n",
       " [1, 4, 1, 5, 1, 2, 3],\n",
       " [1, 2, 2, 1, 1, 4],\n",
       " [1, 5, 1],\n",
       " [1, 1, 4, 1, 1, 4],\n",
       " [1, 1, 1, 1],\n",
       " [1, 1, 1, 2, 2, 1, 4, 1, 1],\n",
       " [1, 1, 5],\n",
       " [4, 1],\n",
       " [2, 2, 2, 4, 1, 1],\n",
       " [5, 1],\n",
       " [2, 2, 1, 2],\n",
       " [1, 2, 2, 1, 1, 1],\n",
       " [1, 4, 1],\n",
       " [1, 2, 2],\n",
       " [5, 2, 1, 2, 2],\n",
       " [5, 1],\n",
       " [3, 1],\n",
       " [5, 1, 2],\n",
       " [1, 2, 1, 1, 1, 2, 1, 4, 2],\n",
       " [5, 1],\n",
       " [1, 4, 3, 4],\n",
       " [1, 1, 1, 2, 1, 1],\n",
       " [1, 4, 4, 1, 1, 1],\n",
       " [1, 2, 2, 1, 1],\n",
       " [2, 1, 4, 1, 4],\n",
       " [1, 2, 2, 1, 2, 2],\n",
       " [1, 5, 3, 1],\n",
       " [5, 3, 1, 3, 1, 4, 3, 1],\n",
       " [5, 1, 4, 1, 2],\n",
       " [5, 1],\n",
       " [5, 4, 5, 3, 3, 1, 1, 2],\n",
       " [1, 2, 2, 1, 1],\n",
       " [2, 1, 1],\n",
       " [1, 4, 1],\n",
       " [1, 5, 4, 1],\n",
       " [2, 4, 1, 1, 1, 1, 4],\n",
       " [3, 4, 1],\n",
       " [4, 4, 1, 3],\n",
       " [1, 1, 1, 4, 1],\n",
       " [1, 5, 1, 1, 3, 1],\n",
       " [2, 2, 1],\n",
       " [1, 2, 2],\n",
       " [1, 4, 3, 1],\n",
       " [2, 1, 1, 5],\n",
       " [2, 1, 1, 4, 4],\n",
       " [5, 1, 1, 3, 1, 3],\n",
       " [1, 2, 2, 2, 1],\n",
       " [5, 5, 4],\n",
       " [4, 1, 4],\n",
       " [1, 2],\n",
       " [1, 1, 1, 1, 2, 1, 5, 2],\n",
       " [5, 5, 1, 4, 1],\n",
       " [4, 2, 1],\n",
       " [2, 2],\n",
       " [1, 4, 5, 1, 1],\n",
       " [1, 1, 2],\n",
       " [4, 5, 1, 5],\n",
       " [2, 2, 1, 1, 2, 1],\n",
       " [5, 2],\n",
       " [2, 2, 2],\n",
       " [5, 1, 4, 1, 2, 2, 1],\n",
       " [1, 4, 2],\n",
       " [1, 5, 1, 4],\n",
       " [1, 4, 1, 1, 5],\n",
       " [1, 1, 5, 1, 4],\n",
       " [1, 1, 1, 3, 4, 1, 1],\n",
       " [2, 1, 1, 2],\n",
       " [1, 1],\n",
       " [2, 2, 2],\n",
       " [1, 4, 1, 3, 4],\n",
       " [2, 2, 1, 1, 2],\n",
       " [5, 5, 1, 1, 1, 4, 3, 4],\n",
       " [1, 1, 1, 1, 2, 4],\n",
       " [1, 1, 4, 4],\n",
       " [5, 2, 2, 1, 2],\n",
       " [1, 4, 4],\n",
       " [1, 1, 2, 4, 2, 2, 1],\n",
       " [5, 1],\n",
       " [1, 5, 2, 1],\n",
       " [5, 3, 4, 4],\n",
       " [1, 2, 2],\n",
       " [1, 5, 1, 4, 1],\n",
       " [4, 4, 1, 3],\n",
       " [2, 2, 4, 1, 4, 2],\n",
       " [1, 5, 3],\n",
       " [1, 5, 2, 1, 4],\n",
       " [1]]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_list = []\n",
    "pitchs_ab = []\n",
    "\n",
    "for ab in range(0, pb.shape[0]):\n",
    "    ab_val = pb.at[ab, 'new_ab']\n",
    "    \n",
    "# Case for last pitch of the season is a one pitch at bat\n",
    "    if ab_val == 'yes' and ab == pb.shape[0] - 1:\n",
    "        pitchs_ab = []\n",
    "        pitchs_ab.append(pb.at[ab, 'pitch_val'])\n",
    "        ab_list.append(pitchs_ab)\n",
    "        \n",
    "# Case for last pitch of the season is not a one pitch at bat\n",
    "    elif ab_val == 'no' and ab == pb.shape[0] - 1:\n",
    "        pitchs_ab.append(pb.at[ab, 'pitch_val'])\n",
    "        ab_list.append(pitchs_ab)\n",
    "        \n",
    "# Case for a one pitch at bat that is not the last of the season\n",
    "    elif ab_val == 'yes' and ab == pb.at[ab + 1, 'new_ab'] == 'yes':\n",
    "        pitchs_ab = []\n",
    "        pitchs_ab.append(pb.at[ab, 'pitch_val'])\n",
    "        ab_list.append(pitchs_ab)\n",
    "        \n",
    "# Case for first pitch of an at bat\n",
    "    elif ab_val == 'yes' and pb.at[ab + 1, 'new_ab'] == 'no':\n",
    "        pitchs_ab = []\n",
    "        pitchs_ab.append(pb.at[ab, 'pitch_val'])\n",
    "\n",
    "# Case for pitch that is last pitch of an at bat\n",
    "    elif ab_val == 'no' and pb.at[ab + 1, 'new_ab'] == 'yes':\n",
    "        pitchs_ab.append(pb.at[ab, 'pitch_val'])\n",
    "        ab_list.append(pitchs_ab)\n",
    "\n",
    "# Case for pitch that is neither the first nor last pitch in an at bat\n",
    "    else:\n",
    "        pitchs_ab.append(pb.at[ab, 'pitch_val'])\n",
    "\n",
    "ab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pb.pitch_val.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)\n",
    "val_data = data[2000:2770]\n",
    "val_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 1s/step - loss: 6.5558 - val_loss: 7.7575\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 6.3922 - val_loss: 7.6140\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 5.8271 - val_loss: 7.4721\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 7.0809 - val_loss: 7.3304\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 1.5529 - val_loss: 7.2000\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 7.5145 - val_loss: 7.0677\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.5407 - val_loss: 6.9342\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 9.1825 - val_loss: 6.7982\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 8.0136 - val_loss: 6.6591\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 6.7977 - val_loss: 6.5195\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 6.6455 - val_loss: 6.3832\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 2.7915 - val_loss: 6.2507\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 3.1477 - val_loss: 6.1204\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 330ms/step - loss: 6.3526 - val_loss: 5.9840\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 1.1490 - val_loss: 5.8530\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 5.3154 - val_loss: 5.7181\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 7.8017 - val_loss: 5.5763\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 3.7604 - val_loss: 5.4316\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 6.4064 - val_loss: 5.2849\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 3.6404 - val_loss: 5.1404\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 4.0448 - val_loss: 4.9947\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 1.4380 - val_loss: 4.8532\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 4.2209 - val_loss: 4.7097\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 4.5704 - val_loss: 4.5577\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 4.1233 - val_loss: 4.4011\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 4.1277 - val_loss: 4.2436\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 7.4085 - val_loss: 4.0826\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 3.7028 - val_loss: 3.9224\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 3.0995 - val_loss: 3.7609\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 5.1025 - val_loss: 3.5956\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 4.0957 - val_loss: 3.4252\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.9168 - val_loss: 3.2613\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 2.6863 - val_loss: 3.1001\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.5360 - val_loss: 2.9380\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 4.5174 - val_loss: 2.7693\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 2.4226 - val_loss: 2.6013\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.7695 - val_loss: 2.4401\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 5.1437 - val_loss: 2.2930\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.5727 - val_loss: 2.1332\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 1.7425 - val_loss: 1.9807\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 2.7332 - val_loss: 1.8253\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 4.1577 - val_loss: 1.6798\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 3.4185 - val_loss: 1.5545\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 3.7619 - val_loss: 1.4456\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 1.7846 - val_loss: 1.3673\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 2.9465 - val_loss: 1.3274\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 3.7239 - val_loss: 1.3187\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 1.0082 - val_loss: 1.3239\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 3.4316 - val_loss: 1.3206\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 2.8125 - val_loss: 1.3278\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 5.1452 - val_loss: 1.3404\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 4.3859 - val_loss: 1.3644\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 3.5705 - val_loss: 1.3939\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.3469 - val_loss: 1.4342\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.6201 - val_loss: 1.4713\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 3.1051 - val_loss: 1.5082\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 1.5719 - val_loss: 1.5485\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 1.7243 - val_loss: 1.5914\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.8931 - val_loss: 1.6193\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 2.0268 - val_loss: 1.6499\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 0.7122 - val_loss: 1.6752\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 3.2847 - val_loss: 1.7077\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 0.2708 - val_loss: 1.7357\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.2665 - val_loss: 1.7582\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 1.9854 - val_loss: 1.7728\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.2648 - val_loss: 1.7862\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 1.0278 - val_loss: 1.7888\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 2.9679 - val_loss: 1.7833\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 2.8358 - val_loss: 1.7699\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 3.6590 - val_loss: 1.7554\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.7554 - val_loss: 1.7275\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 3.1598 - val_loss: 1.6937\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 3.2139 - val_loss: 1.6531\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 2.5024 - val_loss: 1.6250\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 3.6357 - val_loss: 1.6032\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 1.7779 - val_loss: 1.5855\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 1.3643 - val_loss: 1.5699\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 1.9769 - val_loss: 1.5553\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 1.1097 - val_loss: 1.5405\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 2.7384 - val_loss: 1.5249\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 1.5730 - val_loss: 1.5092\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 6.3174 - val_loss: 1.5079\n",
      "Epoch 83/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 189ms/step - loss: 2.4257 - val_loss: 1.5059\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.2907 - val_loss: 1.5064\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 2.2346 - val_loss: 1.5057\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.1603 - val_loss: 1.5087\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.2250 - val_loss: 1.5126\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 2.0237 - val_loss: 1.5103\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 2.6541 - val_loss: 1.5198\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 0.6195 - val_loss: 1.5280\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 1.1776 - val_loss: 1.5338\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.1787 - val_loss: 1.5268\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 1.8087 - val_loss: 1.5232\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 3.0019 - val_loss: 1.5151\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 2.6259 - val_loss: 1.5084\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 1.4001 - val_loss: 1.5057\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 2.1460 - val_loss: 1.5035\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 3.4148 - val_loss: 1.5045\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 0.5307 - val_loss: 1.5049\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.8914 - val_loss: 1.5039\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 3.0342 - val_loss: 1.5054\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.9345 - val_loss: 1.5092\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 3.0372 - val_loss: 1.5112\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.8358 - val_loss: 1.5164\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 2.6945 - val_loss: 1.5248\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 3.0987 - val_loss: 1.5327\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 3.6599 - val_loss: 1.5397\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 3.0756 - val_loss: 1.5478\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 1.7530 - val_loss: 1.5534\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 1.5119 - val_loss: 1.5610\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.3064 - val_loss: 1.5679\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1277 - val_loss: 1.5878\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 2.5417 - val_loss: 1.6127\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 3.1563 - val_loss: 1.6326\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 1.7023 - val_loss: 1.6481\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.6963 - val_loss: 1.6671\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 2.3806 - val_loss: 1.6758\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 1.9447 - val_loss: 1.6809\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 3.0095 - val_loss: 1.6864\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 2.7307 - val_loss: 1.6882\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 3.4842 - val_loss: 1.6988\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.8732 - val_loss: 1.7214\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 1.4671 - val_loss: 1.7412\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 2.1287 - val_loss: 1.7621\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 1.1658 - val_loss: 1.7792\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.1246 - val_loss: 1.7922\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 4.3237 - val_loss: 1.8022\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 5.6439 - val_loss: 1.8076\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 1.1320 - val_loss: 1.8127\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 0.8479 - val_loss: 1.8272\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 3.1365 - val_loss: 1.8363\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 2.8195 - val_loss: 1.8301\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 3.2124 - val_loss: 1.8124\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 4.4158 - val_loss: 1.7902\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 3.4981 - val_loss: 1.7643\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 0.3154 - val_loss: 1.7494\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 2.2113 - val_loss: 1.7373\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.5737 - val_loss: 1.7261\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 2.0593 - val_loss: 1.7153\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 1.1897 - val_loss: 1.7070\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 1.6669 - val_loss: 1.7046\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 3.1627 - val_loss: 1.7047\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 2.8828 - val_loss: 1.7075\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 3.7572 - val_loss: 1.7088\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 2.6252 - val_loss: 1.7106\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 1.9800 - val_loss: 1.7098\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 2.4605 - val_loss: 1.7113\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 2.2189 - val_loss: 1.7163\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 2.8654 - val_loss: 1.7226\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 0.9379 - val_loss: 1.7270\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 1.2733 - val_loss: 1.7320\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 1.5821 - val_loss: 1.7385\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 2.6811 - val_loss: 1.7433\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 1.8492 - val_loss: 1.7460\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 0.4994 - val_loss: 1.7525\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 1.1268 - val_loss: 1.7579\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 1.3523 - val_loss: 1.7661\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 1.4134 - val_loss: 1.7827\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 1.6718 - val_loss: 1.8057\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.9691 - val_loss: 1.8252\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 3.3498 - val_loss: 1.8313\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 3.0203 - val_loss: 1.8322\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.9282 - val_loss: 1.8296\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 0.3233 - val_loss: 1.8287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 2.1897 - val_loss: 1.8234\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 1.0698 - val_loss: 1.8181\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 3.2449 - val_loss: 1.8083\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 1.7411 - val_loss: 1.8098\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.1722 - val_loss: 1.8128\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 3.8892 - val_loss: 1.8136\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 0.8366 - val_loss: 1.8167\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 2.9315 - val_loss: 1.8094\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.2844 - val_loss: 1.8050\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 2.7875 - val_loss: 1.8049\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 1.8829 - val_loss: 1.8071\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 0.2402 - val_loss: 1.8097\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 1.8986 - val_loss: 1.8143\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 1.1548 - val_loss: 1.8201\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 1.5952 - val_loss: 1.8279\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 2.4790 - val_loss: 1.8330\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.6797 - val_loss: 1.8453\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 1.1398 - val_loss: 1.8590\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.5799 - val_loss: 1.8647\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.4293 - val_loss: 1.8642\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 3.6587 - val_loss: 1.8672\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.4285 - val_loss: 1.8714\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 1.8243 - val_loss: 1.8741\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.2609 - val_loss: 1.8621\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.6151 - val_loss: 1.8523\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 0.7038 - val_loss: 1.8472\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 3.3587 - val_loss: 1.8422\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 3.9738 - val_loss: 1.8414\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 4.8688 - val_loss: 1.8428\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 2.8027 - val_loss: 1.8476\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.0284 - val_loss: 1.8579\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.5425 - val_loss: 1.8677\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 3.8005 - val_loss: 1.8720\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 1.9487 - val_loss: 1.8666\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 1.5978 - val_loss: 1.8595\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 3.5221 - val_loss: 1.8564\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.5213 - val_loss: 1.8613\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 2.7457 - val_loss: 1.8659\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 1.1491 - val_loss: 1.8720\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.2831 - val_loss: 1.8820\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.4155 - val_loss: 1.8959\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 1.0580 - val_loss: 1.9102\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.8865 - val_loss: 1.9272\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 1.6495 - val_loss: 1.9420\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 3.5189 - val_loss: 1.9379\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 2.8861 - val_loss: 1.9339\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.2009 - val_loss: 1.9319\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 2.9327 - val_loss: 1.9213\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 2.9383 - val_loss: 1.9129\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 2.4297 - val_loss: 1.9042\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.6602 - val_loss: 1.8942\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 2.0581 - val_loss: 1.8763\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 2.5441 - val_loss: 1.8602\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 1.9771 - val_loss: 1.8552\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 0.7092 - val_loss: 1.8546\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.3711 - val_loss: 1.8572\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 0.3605 - val_loss: 1.8627\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 1.3501 - val_loss: 1.8749\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 2.3967 - val_loss: 1.8859\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 2.1206 - val_loss: 1.8993\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.6573 - val_loss: 1.9042\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.3343 - val_loss: 1.9056\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.5996 - val_loss: 1.9013\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 0.3115 - val_loss: 1.9005\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 1.5400 - val_loss: 1.9055\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 1.6229 - val_loss: 1.9153\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 4.3806 - val_loss: 1.9263\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.8906 - val_loss: 1.9335\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 3.4894 - val_loss: 1.9315\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.0320 - val_loss: 1.9248\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.3715 - val_loss: 1.9064\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 2.4995 - val_loss: 1.8880\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.2725 - val_loss: 1.8705\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 3.3337 - val_loss: 1.8552\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 2.4352 - val_loss: 1.8414\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 0.4856 - val_loss: 1.8339\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 1.6762 - val_loss: 1.8270\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 2.0679 - val_loss: 1.8206\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.8730 - val_loss: 1.8146\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 3.5809 - val_loss: 1.8048\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 3.0734 - val_loss: 1.7999\n",
      "Epoch 246/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 204ms/step - loss: 3.3006 - val_loss: 1.7947\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.5339 - val_loss: 1.7924\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 2.0934 - val_loss: 1.7911\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.3009 - val_loss: 1.7897\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.1789 - val_loss: 1.7860\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.2653 - val_loss: 1.7839\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.3677 - val_loss: 1.7836\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 2.8808 - val_loss: 1.7820\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 2.0971 - val_loss: 1.7817\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.8228 - val_loss: 1.7817\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.1611 - val_loss: 1.7832\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.6585 - val_loss: 1.7871\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 4.2507 - val_loss: 1.7906\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.2350 - val_loss: 1.8003\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 2.4277 - val_loss: 1.8184\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 0.9883 - val_loss: 1.8478\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 3.1129 - val_loss: 1.8728\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 3.2254 - val_loss: 1.8991\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 3.1002 - val_loss: 1.9165\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.4354 - val_loss: 1.9381\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.3204 - val_loss: 1.9605\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.8850 - val_loss: 1.9698\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.3925 - val_loss: 1.9736\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 1.7025 - val_loss: 1.9773\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 3.4244 - val_loss: 1.9834\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 4.6271 - val_loss: 1.9786\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.8323 - val_loss: 1.9823\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 1.4757 - val_loss: 1.9832\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 0.5769 - val_loss: 1.9863\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 2.7315 - val_loss: 1.9766\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 3.3044 - val_loss: 1.9665\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 2.5278 - val_loss: 1.9504\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 2.7175 - val_loss: 1.9372\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 0.8067 - val_loss: 1.9350\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 1.8644 - val_loss: 1.9366\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 1.9914 - val_loss: 1.9352\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 1.4228 - val_loss: 1.9406\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 1.9757 - val_loss: 1.9482\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 3.2355 - val_loss: 1.9426\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 1.2779 - val_loss: 1.9424\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1964 - val_loss: 1.9383\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 3.8752 - val_loss: 1.9304\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 2.9493 - val_loss: 1.9147\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 1.0322 - val_loss: 1.9038\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 1.5972 - val_loss: 1.8938\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 2.6364 - val_loss: 1.8834\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 1.5236 - val_loss: 1.8774\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.5102 - val_loss: 1.8739\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 1.9946 - val_loss: 1.8732\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.5980 - val_loss: 1.8760\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.6076 - val_loss: 1.8830\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 0.8791 - val_loss: 1.8947\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 3.1332 - val_loss: 1.9054\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 0.4828 - val_loss: 1.9206\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 2.0427 - val_loss: 1.9412\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 293ms/step - loss: 2.1604 - val_loss: 1.9643\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 321ms/step - loss: 2.1918 - val_loss: 1.9915\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 279ms/step - loss: 2.9755 - val_loss: 2.0024\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.2972 - val_loss: 2.0172\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 1.2121 - val_loss: 2.0356\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 1.2043 - val_loss: 2.0510\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 1.3670 - val_loss: 2.0626\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 1.3466 - val_loss: 2.0737\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 305ms/step - loss: 2.3313 - val_loss: 2.0781\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 2.3152 - val_loss: 2.0762\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 3.5376 - val_loss: 2.0712\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.3592 - val_loss: 2.0712\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 2.3023 - val_loss: 2.0717\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.4278 - val_loss: 2.0729\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 1.3486 - val_loss: 2.0702\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1677 - val_loss: 2.0754\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 1.8049 - val_loss: 2.0721\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 0.9588 - val_loss: 2.0727\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 2.5999 - val_loss: 2.0681\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 4.6100 - val_loss: 2.0432\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 1.1671 - val_loss: 2.0259\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 1.2197 - val_loss: 2.0087\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 2.7742 - val_loss: 1.9839\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 1.9835 - val_loss: 1.9670\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 3.1666 - val_loss: 1.9479\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 1.0067 - val_loss: 1.9343\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 0.4015 - val_loss: 1.9264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 2.0381 - val_loss: 1.9149\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 2.2934 - val_loss: 1.9057\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.8154 - val_loss: 1.8940\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 3.6382 - val_loss: 1.8882\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 3.1089 - val_loss: 1.8830\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.5851 - val_loss: 1.8809\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 1.9237 - val_loss: 1.8768\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.8176 - val_loss: 1.8770\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 1.9341 - val_loss: 1.8764\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.9430 - val_loss: 1.8777\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 2.1195 - val_loss: 1.8786\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 1.9076 - val_loss: 1.8807\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 2.6663 - val_loss: 1.8829\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.4670 - val_loss: 1.8846\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 3.4575 - val_loss: 1.8862\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.2055 - val_loss: 1.8859\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 2.4141 - val_loss: 1.8904\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 1.2070 - val_loss: 1.8997\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 0.8788 - val_loss: 1.9139\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.9101 - val_loss: 1.9230\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 1.8868 - val_loss: 1.9350\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 1.3212 - val_loss: 1.9502\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.6163 - val_loss: 1.9684\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.4544 - val_loss: 1.9770\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.9388 - val_loss: 1.9868\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 0.9377 - val_loss: 2.0086\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.7839 - val_loss: 2.0267\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 3.1015 - val_loss: 2.0411\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 1.7190 - val_loss: 2.0555\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 3.1740 - val_loss: 2.0523\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.4885 - val_loss: 2.0562\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 1.4609 - val_loss: 2.0611\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.1745 - val_loss: 2.0608\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 2.2464 - val_loss: 2.0588\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 3.2120 - val_loss: 2.0506\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.9132 - val_loss: 2.0447\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 1.6760 - val_loss: 2.0401\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.8646 - val_loss: 2.0293\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 1.7508 - val_loss: 2.0155\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.1176 - val_loss: 2.0070\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.0781 - val_loss: 1.9915\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 3.4251 - val_loss: 1.9748\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 0.4437 - val_loss: 1.9651\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 1.3667 - val_loss: 1.9564\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 1.6483 - val_loss: 1.9483\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 1.5745 - val_loss: 1.9458\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 3.3388 - val_loss: 1.9434\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 1.7916 - val_loss: 1.9454\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 1.8436 - val_loss: 1.9522\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 1.0046 - val_loss: 1.9608\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.0341 - val_loss: 1.9685\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.7441 - val_loss: 1.9804\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.2468 - val_loss: 1.9907\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 1.9064 - val_loss: 1.9969\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 1.9460 - val_loss: 2.0073\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 2.5623 - val_loss: 2.0118\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 1.2425 - val_loss: 2.0221\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 1.2600 - val_loss: 2.0364\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.7897 - val_loss: 2.0381\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 0.7459 - val_loss: 2.0461\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.1974 - val_loss: 2.0524\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 1.6377 - val_loss: 2.0586\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 0.4655 - val_loss: 2.0710\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 0.4658 - val_loss: 2.0892\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 1.4315 - val_loss: 2.1024\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.6920 - val_loss: 2.1086\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 2.3166 - val_loss: 2.1089\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.6412 - val_loss: 2.1136\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 0.3729 - val_loss: 2.1229\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.2678 - val_loss: 2.1252\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.8929 - val_loss: 2.1297\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 1.8289 - val_loss: 2.1337\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 1.8971 - val_loss: 2.1317\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.0474 - val_loss: 2.1289\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.6566 - val_loss: 2.1181\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 1.4948 - val_loss: 2.1027\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 1.0293 - val_loss: 2.0918\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 1.6873 - val_loss: 2.0699\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 1.3022 - val_loss: 2.0483\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 1.6696 - val_loss: 2.0318\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 1.8190 - val_loss: 2.0176\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 2.1213 - val_loss: 1.9946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 1.2892 - val_loss: 1.9825\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 1.8558 - val_loss: 1.9733\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 1.0973 - val_loss: 1.9711\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.3811 - val_loss: 1.9612\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 1.5061 - val_loss: 1.9550\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.3175 - val_loss: 1.9528\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 3.0436 - val_loss: 1.9504\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1835 - val_loss: 1.9485\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.2964 - val_loss: 1.9414\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 1.4660 - val_loss: 1.9353\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 0.5437 - val_loss: 1.9363\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.4757 - val_loss: 1.9314\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 2.5202 - val_loss: 1.9231\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 2.0727 - val_loss: 1.9148\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 3.1407 - val_loss: 1.9101\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.3822 - val_loss: 1.9121\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 1.6369 - val_loss: 1.9166\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 1.6309 - val_loss: 1.9220\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.1419 - val_loss: 1.9260\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.8648 - val_loss: 1.9302\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 1.6305 - val_loss: 1.9377\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 2.9430 - val_loss: 1.9404\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 1.9327 - val_loss: 1.9417\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 1.9052 - val_loss: 1.9489\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 0.8797 - val_loss: 1.9581\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.3748 - val_loss: 1.9765\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 4.2818 - val_loss: 1.9750\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 2.3639 - val_loss: 1.9705\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.6874 - val_loss: 1.9593\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.6132 - val_loss: 1.9407\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 0.5314 - val_loss: 1.9320\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 2.3909 - val_loss: 1.9203\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 2.2182 - val_loss: 1.9050\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.5011 - val_loss: 1.8949\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.2148 - val_loss: 1.8835\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 1.7192 - val_loss: 1.8752\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 1.8917 - val_loss: 1.8706\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 1.3528 - val_loss: 1.8728\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.1674 - val_loss: 1.8742\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 1.9430 - val_loss: 1.8755\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 2.5774 - val_loss: 1.8841\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 1.4565 - val_loss: 1.8943\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 1.2999 - val_loss: 1.9090\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 2.5858 - val_loss: 1.9251\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 1.3403 - val_loss: 1.9480\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.0388 - val_loss: 1.9756\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.4684 - val_loss: 1.9998\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 0.7697 - val_loss: 2.0342\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 2.8946 - val_loss: 2.0531\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 1.9862 - val_loss: 2.0668\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 1.9645 - val_loss: 2.0673\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 1.5563 - val_loss: 2.0678\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 1.8685 - val_loss: 2.0660\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 1.0418 - val_loss: 2.0688\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 2.4646 - val_loss: 2.0636\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 1.7745 - val_loss: 2.0635\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 1.1460 - val_loss: 2.0588\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 2.7774 - val_loss: 2.0508\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 1.6011 - val_loss: 2.0453\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 1.2926 - val_loss: 2.0491\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 1.9566 - val_loss: 2.0456\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.8236 - val_loss: 2.0447\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 1.7315 - val_loss: 2.0436\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 3.0611 - val_loss: 2.0445\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 2.9402 - val_loss: 2.0344\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 3.3133 - val_loss: 2.0210\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.8357 - val_loss: 1.9993\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 3.0982 - val_loss: 1.9691\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 1.5341 - val_loss: 1.9478\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 1.7986 - val_loss: 1.9324\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.7675 - val_loss: 1.9237\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 1.1330 - val_loss: 1.9201\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.8305 - val_loss: 1.9117\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 2.3691 - val_loss: 1.9081\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 1.8489 - val_loss: 1.9075\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 1.4763 - val_loss: 1.9053\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 2.4021 - val_loss: 1.9011\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.4838 - val_loss: 1.8976\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 0.8829 - val_loss: 1.8971\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 1.8412 - val_loss: 1.8998\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 1.4794 - val_loss: 1.9090\n",
      "Epoch 491/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 198ms/step - loss: 3.3030 - val_loss: 1.9239\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 1.8935 - val_loss: 1.9454\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 1.5022 - val_loss: 1.9753\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 1.5105 - val_loss: 2.0143\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 0.8798 - val_loss: 2.0665\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 1.2473 - val_loss: 2.1280\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 3.5646 - val_loss: 2.1798\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 1.0660 - val_loss: 2.2301\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 1.8066 - val_loss: 2.2826\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 2.2617 - val_loss: 2.3309\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fde8bc806d0>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# univariate one step problem with lstm\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "\n",
    "# define dataset\n",
    "series = array(data)\n",
    "series_val = array(val_data)\n",
    "\n",
    "\n",
    "# reshape to [10, 1]\n",
    "n_features = 1\n",
    "series = series.reshape((len(series), n_features))\n",
    "series_val = series_val.reshape((len(series_val), n_features))\n",
    "\n",
    "# define generator\n",
    "n_input = 3\n",
    "generator = TimeseriesGenerator(series, series, length=n_input, batch_size=8)\n",
    "gen_val = TimeseriesGenerator(series_val, series_val, length=n_input, batch_size=8)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, activation='relu', input_shape=(n_input, n_features)))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit_generator(generator, steps_per_epoch=1, epochs=500, verbose=1, validation_data=gen_val)\n",
    "# # make a one step prediction out of sample\n",
    "# x_input = array([9, 10]).reshape((1, n_input, n_features))\n",
    "# yhat = model.predict(x_input, verbose=0)\n",
    "# print(yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 412us/step\n",
      "[[1.7818307]\n",
      " [1.7904371]\n",
      " [1.8187258]]\n"
     ]
    }
   ],
   "source": [
    "x_input = array([2, 1, 1]).reshape((1, n_input, n_features))\n",
    "yhat = model.predict(batch_0[0], verbose=1)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2]],\n",
       "\n",
       "       [[1]],\n",
       "\n",
       "       [[1]]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_0[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Figure out y-val OHE\n",
    "\n",
    "SEQUENCE_LENGTH = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - SEQUENCE_LENGTH, step):\n",
    "    sentences.append(text[i: i + SEQUENCE_LENGTH])\n",
    "    next_chars.append(text[i + SEQUENCE_LENGTH])\n",
    "print(f'num training examples: {len(sentences)}')\n",
    "\n",
    "\n",
    "text is data\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.array([[i] for i in data])\n",
    "targets = np.array([[i] for i in data])\n",
    "data_gen = TimeseriesGenerator(data, targets,\n",
    "                               length=3, sampling_rate=1,\n",
    "                               batch_size=3)\n",
    "# assert len(data_gen) == 20\n",
    "# batch_0 = data_gen[0]\n",
    "# x, y = batch_0\n",
    "# assert np.array_equal(x,\n",
    "#                       np.array([[[0], [2], [4], [6], [8]],\n",
    "#                                 [[1], [3], [5], [7], [9]]]))\n",
    "# assert np.array_equal(y,\n",
    "#                       np.array([[10], [11]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "923"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_0 = data_gen[0]\n",
    "# x, y = batch_0\n",
    "# # x[0, :, :], y.shape\n",
    "\n",
    "# batch_1 = data_gen[1]\n",
    "# x_1, y_1 = batch_1\n",
    "# x_1, y_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape = (3, 1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_21 (LSTM)               (None, 128)               66560     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 66,689\n",
      "Trainable params: 66,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_13 to have 2 dimensions, but got array with shape (3, 1, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-511bea698bfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model.fit_generator(generator, steps_per_epoch=1, epochs=200, verbose=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1506\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m             class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    133\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    136\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_13 to have 2 dimensions, but got array with shape (3, 1, 1)"
     ]
    }
   ],
   "source": [
    "model.fit_generator(data_gen, steps_per_epoch=1, epochs=20, shuffle=False)\n",
    "\n",
    "# model.fit_generator(generator, steps_per_epoch=1, epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p1</th>\n",
       "      <th>p2</th>\n",
       "      <th>p3</th>\n",
       "      <th>p4</th>\n",
       "      <th>p5</th>\n",
       "      <th>p6</th>\n",
       "      <th>p7</th>\n",
       "      <th>p8</th>\n",
       "      <th>p9</th>\n",
       "      <th>p10</th>\n",
       "      <th>p11</th>\n",
       "      <th>p12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>633</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>634 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     p1  p2  p3  p4  p5  p6  p7  p8  p9  p10  p11  p12\n",
       "0     1   1   1   0   0   0   0   0   0    0    0    0\n",
       "1     2   1   1   3   1   4   3   0   0    0    0    0\n",
       "2     1   4   4   0   0   0   0   0   0    0    0    0\n",
       "3     5   1   1   4   0   0   0   0   0    0    0    0\n",
       "4     1   2   1   4   5   0   0   0   0    0    0    0\n",
       "..   ..  ..  ..  ..  ..  ..  ..  ..  ..  ...  ...  ...\n",
       "629   4   4   1   3   0   0   0   0   0    0    0    0\n",
       "630   2   2   4   1   4   2   0   0   0    0    0    0\n",
       "631   1   5   3   0   0   0   0   0   0    0    0    0\n",
       "632   1   5   2   1   4   0   0   0   0    0    0    0\n",
       "633   1   0   0   0   0   0   0   0   0    0    0    0\n",
       "\n",
       "[634 rows x 12 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "at_bats = pd.DataFrame(data = ab_list)\n",
    "at_bats = at_bats.fillna(value=0)\n",
    "at_bats.shape[1] - 1\n",
    "cols = ['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8', 'p9', 'p10', 'p11', 'p12']\n",
    "at_bats.columns = cols\n",
    "at_bats = at_bats.applymap(lambda v: int(v))\n",
    "at_bats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 4,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 4,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 5,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 4,\n",
       " 2,\n",
       " 1,\n",
       " 5,\n",
       " 1,\n",
       " 5,\n",
       " 5,\n",
       " 1,\n",
       " 4,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 1,\n",
       " 5,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 4,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " 3,\n",
       " 1,\n",
       " 2,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_list = []\n",
    "for i in range(0, at_bats.shape[0]):\n",
    "#     print(i)\n",
    "    flat_list.append(at_bats.iloc[0, :].tolist())\n",
    "\n",
    "\n",
    "flat_list = [item for sublist in ab_list for item in sublist]\n",
    "flat_list\n",
    "# at_bats.iloc[0, :].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prepare the dataset of input to output pairs encoded as integers\n",
    "# raw_text = flat_list\n",
    "# n_chars = len(raw_text)\n",
    "\n",
    "# seq_length = 1\n",
    "# dataX = []\n",
    "# dataY = []\n",
    "# for i in range(0, n_chars - seq_length):\n",
    "#     seq_in = raw_text[i:i + seq_length]\n",
    "#     seq_out = raw_text[i + seq_length]\n",
    "#     dataX.append([char_to_int[char] for char in seq_in])\n",
    "#     dataY.append(char_to_int[seq_out])\n",
    "# n_patterns = len(dataX)\n",
    "# print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_list = [item for sublist in ab_list for item in sublist]\n",
    "flat_list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load ascii text and covert to lowercase\n",
    "# filename = 'wonderland.txt'\n",
    "# raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "# raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create mapping of unique chars to integers, and a reverse mapping\n",
    "# chars = sorted(list(set(raw_text)))\n",
    "# char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # summarize the loaded data\n",
    "# n_chars = len(raw_text)\n",
    "# n_vocab = len(chars)\n",
    "# print(\"Total Characters: \", n_chars)\n",
    "# print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'pitch_val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fe2941f6648a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m## Jam my data into this tutorial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdataX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mat_bats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'p1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p3'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p4'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p6'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p7'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p9'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p10'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'p11'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdataY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mat_bats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpitch_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mn_patterns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'pitch_val'"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 11\n",
    "# dataX = []\n",
    "# dataY = []\n",
    "# for i in range(0, n_chars - seq_length, 1):\n",
    "#     seq_in = raw_text[i:i + seq_length]\n",
    "#     seq_out = raw_text[i + seq_length]\n",
    "#     dataX.append([char_to_int[char] for char in seq_in])\n",
    "#     dataY.append(char_to_int[seq_out])\n",
    "# n_patterns = len(dataX)\n",
    "# print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "## Jam my data into this tutorial\n",
    "dataX = np.array(at_bats[['p1', 'p2', 'p3', 'p4', 'p5', 'p6', 'p7', 'p8', 'p9', 'p10', 'p11']])\n",
    "dataY = at_bats.pitch_val\n",
    "\n",
    "n_patterns = len(dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_patterns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f9b015624260>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reshape X to be [samples, time steps, features]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_patterns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_patterns' is not defined"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "X.shape, len(X), len(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "# X = X / float(n_vocab)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-72d8667fb6bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# one hot encode the output variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/utils/np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(y, num_classes, dtype)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mcategorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2666\u001b[0m     \"\"\"\n\u001b[1;32m   2667\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0;32m-> 2668\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   2669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    }
   ],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "y = dataY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-05984396ac70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define the LSTM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "# filename = \"weights-improvement-47-1.2219-bigger.hdf5\"\n",
    "# model.load_weights(filename)\n",
    "# model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'int_to_char' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-72cfffde8be6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Seed:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-72cfffde8be6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Seed:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'int_to_char' is not defined"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-2a39adc378d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'n_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  163780\n",
      "Total Vocab:  58\n",
      "Total Patterns:  163680\n",
      "Epoch 1/3\n",
      "163680/163680 [==============================] - 549s 3ms/step - loss: 2.8930\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.89304, saving model to weights-improvement-01-2.8930.hdf5\n",
      "Epoch 2/3\n",
      "163680/163680 [==============================] - 544s 3ms/step - loss: 2.6197\n",
      "\n",
      "Epoch 00002: loss improved from 2.89304 to 2.61969, saving model to weights-improvement-02-2.6197.hdf5\n",
      "Epoch 3/3\n",
      "163680/163680 [==============================] - 534s 3ms/step - loss: 2.4504\n",
      "\n",
      "Epoch 00003: loss improved from 2.61969 to 2.45041, saving model to weights-improvement-03-2.4504.hdf5\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'weights-improvement-47-1.2219-bigger.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-60c84eea4d05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;31m## load the network weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"weights-improvement-47-1.2219-bigger.hdf5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 492\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch, reshape)\u001b[0m\n\u001b[1;32m   1219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mh5py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`load_weights` requires h5py.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m                 \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'weights-improvement-47-1.2219-bigger.hdf5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "\n",
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=3, batch_size=128, callbacks=callbacks_list)\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "## load the network weights\n",
    "filename = \"weights-improvement-47-1.2219-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', workers = -1)\n",
    "\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hack tutorial code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project gutenberg's alice's adventures in wonderland, by lewis carroll\n",
      "\n",
      "this ebook is for the use of\n",
      "roject gutenberg's alice's adventures in wonderland, by lewis carroll\n",
      "\n",
      "this ebook is for the use of \n",
      "oject gutenberg's alice's adventures in wonderland, by lewis carroll\n",
      "\n",
      "this ebook is for the use of a\n",
      "ject gutenberg's alice's adventures in wonderland, by lewis carroll\n",
      "\n",
      "this ebook is for the use of an\n",
      "ect gutenberg's alice's adventures in wonderland, by lewis carroll\n",
      "\n",
      "this ebook is for the use of any\n",
      "ct gutenberg's alice's adventures in wonderland, by lewis carroll\n",
      "\n",
      "this ebook is for the use of anyo\n",
      "t gutenberg's alice's adventures in wonderland, by lewis carroll\n",
      "\n",
      "this ebook is for the use of anyon\n",
      " gutenberg's alice's adventures in wonderland, by lewis carroll\n",
      "\n",
      "this ebook is for the use of anyone\n",
      "gutenberg's alice's adventures in wonderland, by lewis carroll\n",
      "\n",
      "this ebook is for the use of anyone \n",
      "utenberg's alice's adventures in wonderland, by lewis carroll\n",
      "\n",
      "this ebook is for the use of anyone a\n",
      "Total Patterns:  163730\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 10):\n",
    "    seq_in = raw_text[i: i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    print(seq_in)\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Larger LSTM network and generate text\n",
    "import sys\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "# # load ascii text and covert to lowercase\n",
    "# filename = \"wonderland.txt\"\n",
    "# raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "# raw_text = raw_text.lower()\n",
    "\n",
    "\n",
    "# # create mapping of unique chars to integers, and a reverse mapping\n",
    "# chars = sorted(list(set(raw_text)))\n",
    "# char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "# int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "\n",
    "# # summarize the loaded data\n",
    "# n_chars = len(raw_text)\n",
    "# n_vocab = len(chars)\n",
    "# print(\"Total Characters: \", n_chars)\n",
    "# print(\"Total Vocab: \", n_vocab)\n",
    "\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 11\n",
    "dataX = []\n",
    "dataY = []\n",
    "\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i: i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)\n",
    "\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "\n",
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X, y, epochs=3, batch_size=128, callbacks=callbacks_list)\n",
    "\n",
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "## load the network weights\n",
    "filename = \"weights-improvement-47-1.2219-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', workers = -1)\n",
    "\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "\n",
    "\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fresh start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1, 3, 1, 4, 3, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# pad sequence\n",
    "padded = pad_sequences(ab_list, padding='post')\n",
    "padded[1]\n",
    "\n",
    "# Max pitches as rows, types of pitches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [2, 1, 1, 3, 1, 4, 3, 0, 0, 0, 0, 0],\n",
       "       [1, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [5, 1, 1, 4, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 2, 1, 4, 5, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_shape = numpy.reshape(X_train, \n",
    "#                               (X_train[0], X_train[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-d1c563447463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define the LSTM model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['outs_when_up', 'inning', 'pitch_number', 'opp_score', 'nats_score',\n",
       "       'nats_home1_away0', 'pitch_season', 'pitch_game', 'pitch_bat_gm',\n",
       "       'total_pitches', 'abs', 'whiffs', 'swings', 'takes', 'k', 'walk',\n",
       "       'single', 'double', 'triple', 'hr', 'line_drive', 'ground_ball',\n",
       "       'fly_ball', 'popup', 'rbi', 'sac', 'ba', 'slg', 'iso', 'babip',\n",
       "       'stand_r1', 'if_standard', 'if_strategic', 'of_strategic', '0_1', '0_2',\n",
       "       '1_0', '1_1', '1_2', '2_0', '2_1', '2_2', '3_0', '3_1', '3_2',\n",
       "       'fb:0_sb:0_tb:1', 'fb:0_sb:1_tb:0', 'fb:0_sb:1_tb:1', 'fb:1_sb:0_tb:0',\n",
       "       'fb:1_sb:0_tb:1', 'fb:1_sb:1_tb:0', 'fb:1_sb:1_tb:1'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ba, slg as additional columns for example\n",
    "\n",
    "# Take look at gated recurrent units, better with less data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1939"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fresh start 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://medium.com/@curiousily/making-a-predictive-keyboard-using-recurrent-neural-networks-tensorflow-for-hackers-part-v-3f238d824218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2770"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num training examples: 2767\n"
     ]
    }
   ],
   "source": [
    "text = data\n",
    "\n",
    "SEQUENCE_LENGTH = 3 #make pitches OHE, add five seasons, try gated recurrent networks, mind the input layer value\n",
    "# Add innings as OHE columns or use ordinal values, number of pitches for batter, \n",
    "step = 1\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - SEQUENCE_LENGTH, step):\n",
    "    sentences.append(text[i: i + SEQUENCE_LENGTH])\n",
    "    next_chars.append(text[i + SEQUENCE_LENGTH])\n",
    "print(f'num training examples: {len(sentences)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1], [1, 1, 2], [1, 2, 1], [2, 1, 1], [1, 1, 3]]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 1, 3, 1]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_chars[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### char_indicies\n",
    "\n",
    "X = np.zeros((len(sentences), SEQUENCE_LENGTH, 6), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), 6), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char] = 1\n",
    "    y[i, next_chars[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2767, 3, 6)"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False, False, False, False],\n",
       "       [False, False,  True, False, False, False],\n",
       "       [False,  True, False, False, False, False]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True, False, False, False, False])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4, 5]), array([1338,  573,  214,  401,  241]))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(next_chars, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4835561980484279"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1338/len(next_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=(SEQUENCE_LENGTH, 6)))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "# model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2490 samples, validate on 277 samples\n",
      "Epoch 1/20\n",
      "2490/2490 [==============================] - 1s 497us/step - loss: 1.4938 - accuracy: 0.4707 - val_loss: 1.4245 - val_accuracy: 0.4368\n",
      "Epoch 2/20\n",
      "2490/2490 [==============================] - 0s 137us/step - loss: 1.3452 - accuracy: 0.4888 - val_loss: 1.4198 - val_accuracy: 0.4368\n",
      "Epoch 3/20\n",
      "2490/2490 [==============================] - 0s 152us/step - loss: 1.3296 - accuracy: 0.4888 - val_loss: 1.3895 - val_accuracy: 0.4368\n",
      "Epoch 4/20\n",
      "2490/2490 [==============================] - 0s 140us/step - loss: 1.3264 - accuracy: 0.4888 - val_loss: 1.4016 - val_accuracy: 0.4368\n",
      "Epoch 5/20\n",
      "2490/2490 [==============================] - 0s 138us/step - loss: 1.3214 - accuracy: 0.4888 - val_loss: 1.3791 - val_accuracy: 0.4368\n",
      "Epoch 6/20\n",
      "2490/2490 [==============================] - 0s 133us/step - loss: 1.3199 - accuracy: 0.4888 - val_loss: 1.3941 - val_accuracy: 0.4368\n",
      "Epoch 7/20\n",
      "2490/2490 [==============================] - 0s 133us/step - loss: 1.3184 - accuracy: 0.4888 - val_loss: 1.3951 - val_accuracy: 0.4368\n",
      "Epoch 8/20\n",
      "2490/2490 [==============================] - 0s 184us/step - loss: 1.3190 - accuracy: 0.4888 - val_loss: 1.3821 - val_accuracy: 0.4368\n",
      "Epoch 9/20\n",
      "2490/2490 [==============================] - 0s 151us/step - loss: 1.3152 - accuracy: 0.4888 - val_loss: 1.3858 - val_accuracy: 0.4368\n",
      "Epoch 10/20\n",
      "2490/2490 [==============================] - 0s 157us/step - loss: 1.3141 - accuracy: 0.4888 - val_loss: 1.3762 - val_accuracy: 0.4368\n",
      "Epoch 11/20\n",
      "2490/2490 [==============================] - 0s 139us/step - loss: 1.3145 - accuracy: 0.4888 - val_loss: 1.3860 - val_accuracy: 0.4368\n",
      "Epoch 12/20\n",
      "2490/2490 [==============================] - 0s 146us/step - loss: 1.3140 - accuracy: 0.4888 - val_loss: 1.3794 - val_accuracy: 0.4368\n",
      "Epoch 13/20\n",
      "2490/2490 [==============================] - 0s 151us/step - loss: 1.3147 - accuracy: 0.4888 - val_loss: 1.3851 - val_accuracy: 0.4368\n",
      "Epoch 14/20\n",
      "2490/2490 [==============================] - 0s 167us/step - loss: 1.3124 - accuracy: 0.4888 - val_loss: 1.3815 - val_accuracy: 0.4368\n",
      "Epoch 15/20\n",
      "2490/2490 [==============================] - 0s 163us/step - loss: 1.3132 - accuracy: 0.4888 - val_loss: 1.3822 - val_accuracy: 0.4368\n",
      "Epoch 16/20\n",
      "2490/2490 [==============================] - 0s 164us/step - loss: 1.3112 - accuracy: 0.4888 - val_loss: 1.3802 - val_accuracy: 0.4368\n",
      "Epoch 17/20\n",
      "2490/2490 [==============================] - 0s 175us/step - loss: 1.3122 - accuracy: 0.4888 - val_loss: 1.4004 - val_accuracy: 0.4368\n",
      "Epoch 18/20\n",
      "2490/2490 [==============================] - 0s 145us/step - loss: 1.3137 - accuracy: 0.4884 - val_loss: 1.3743 - val_accuracy: 0.4368\n",
      "Epoch 19/20\n",
      "2490/2490 [==============================] - 0s 138us/step - loss: 1.3083 - accuracy: 0.4888 - val_loss: 1.3807 - val_accuracy: 0.4368\n",
      "Epoch 20/20\n",
      "2490/2490 [==============================] - 0s 172us/step - loss: 1.3119 - accuracy: 0.4888 - val_loss: 1.3907 - val_accuracy: 0.4368\n"
     ]
    }
   ],
   "source": [
    "# optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X, y, validation_split=0.1, batch_size=32, epochs=20, shuffle=True).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.30174584e-04, 5.27137518e-01, 3.37407291e-01, 2.21930947e-02,\n",
       "        7.38542378e-02, 3.91776524e-02],\n",
       "       [1.93668311e-04, 4.83068764e-01, 2.77090818e-01, 2.86542159e-02,\n",
       "        1.21618517e-01, 8.93739536e-02],\n",
       "       [2.58301268e-04, 4.48310196e-01, 2.94483989e-01, 3.82069983e-02,\n",
       "        1.05401002e-01, 1.13339536e-01],\n",
       "       [2.06669050e-04, 5.10605752e-01, 1.16866253e-01, 9.06104296e-02,\n",
       "        2.09130138e-01, 7.25807846e-02]], dtype=float32)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X[1:5, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False,  True, False, False, False, False],\n",
       "       [False,  True, False, False, False, False],\n",
       "       [False, False, False,  True, False, False],\n",
       "       [False,  True, False, False, False, False]])"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
